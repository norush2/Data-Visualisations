---
title: "tutorial_07"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
Reading in our files and loading required packages
```{r, warning= FALSE }
library(tidyverse)
library(LDAvis)
library(readr)
library(tm)
library(lda)

filelist<-list.files(path="../data/", pattern = ".*.txt",full.names = TRUE)
data<-lapply(filelist,FUN=read.delim)


data <- gsub("'", "", data)  # remove apostrophes
data <- gsub("[[:punct:]]", " ", data)  # replace punctuation with space
data <- gsub("[[:cntrl:]]", " ", data)  # replace control characters with space
data <- gsub("^[[:space:]]+", "", data) # remove whitespace at beginning of documents
data <- gsub("[[:space:]]+$", "", data) # remove whitespace at end of documents
data <- tolower(data)  # force to lowercase
stop_words <- stopwords("SMART")
pp_rev <- data %>%
str_replace_all("'", "") %>%
str_replace_all("[[:punct:][:cntrl:]]", " ") %>%
str_trim %>%
str_to_lower()

# tokenize on space and output as a list:
doc.list <- str_split(pp_rev, "[[:space:]]+")
# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
# remove terms that are stop words or occur fewer than 5 times:
del <- names(term.table) %in% stop_words | term.table < 5
term.table <- term.table[!del]
vocab <- names(term.table)
# now put the documents into the format required by the lda package:
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)

# Compute some statistics related to the data set:
D <- length(documents) # number of documents is 73
W <- length(vocab) # number of terms in the vocab
doc.length <- sapply(documents, function(x) sum(x[2, ]))
# number of tokens per document [59, 91, 81, 67, 74, ...]
N <- sum(doc.length) # total number of tokens in the data (5137L)
term.frequency <- as.integer(term.table)

K <- 20
G <- 5000
alpha <- 0.02
eta <- 0.02
# Fit the model:

set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1 # abou


#model visualisation
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
TextReviews <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
# create the JSON object to feed the visualization:
json <- createJSON(phi = TextReviews$phi,
theta = TextReviews$theta,
doc.length = TextReviews$doc.length,
vocab = TextReviews$vocab,
term.frequency = TextReviews$term.frequency)

#plot(fit$log.likelihoods[1,],type = "l")
#These gives us the document numbers that are closely related to the topic
top.topic.documents(fit$document_sums,10)
df<-as.data.frame(top.topic.documents(fit$document_sums,10))
#Top words for the topics
top.topic.words(fit$topics,10)
serVis(json, out.dir = 'vis', privacy.file_unique_origin=TRUE)
```


Hierarchical clustering

With reference to what we learnt in topic 6,  we first get a 20 x 20 matrix of the correlation values
With this matrix, we pivot it longer to form a dataframe of 20 rows and 2 columns.


```{r}
library(MASS)
df2<-df%>%
  cor(use="pair")
topics_df<-as.data.frame(df2)%>%
  mutate(var1 = row.names(df2))%>%
  pivot_longer(V1:V20,names_to="var2",values_to="correlation")

ggplot(topics_df) +
  geom_tile(aes(x=var1, y=var2, fill=correlation)) +
  theme(axis.text.x=element_text(angle=90, 
                                 vjust=0, hjust=1))

### Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
  cormat[upper.tri(cormat)] <- NA
  return(cormat)
}
### Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

upper_tri <- get_upper_tri(cormat)
upper_tri
melted_cormat <- melt(upper_tri, na.rm = TRUE)

### Heatmap
ggplot(data = topics_df, aes(x=var1, y=var2, fill=correlation))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))

topics_df2 <- mutate (topics_df,
var1 = factor(var1,levels=row.names(df2)[ord]),
var2 = factor(var2 ,levels=row.names(df2)[ord]))

ggplot (topics_df2) +
geom_tile (aes(x= var1,y= var2 ,fill=correlation )) +
scale_fill_gradient2 () +
theme ( axis.text.x = element_text (angle =90 , vjust =0 ,
hjust=1))
```

Multi-Dimensional Scaling
```{r}
dist<-as.dist((1 - df2)/2)
hc <-hclust(dist)
plot(as.dendrogram(hc))
abline(h=0.2)


mds2 <- MASS::sammon(dist, k = 2)
grps <- as.factor(cutree(hc, k=4))
mds_df <- data.frame(mds2$points) %>% 
  mutate(label = row.names(mds2$points), Cluster=grps) %>% 
  rename('Var.1' = 'X1', 'Var.2'='X2') 

```



```{r}
ggplot(mds_df) + 
  geom_text(aes(x=Var.1, y=Var.2, label=label, col=Cluster), 
            show.legend = TRUE) +
  labs(title="MDS Output for Text Analysis Data", 
  subtitle = "Colours denote hierarchical clustering output with K=4") 



# Install
install.packages("tm")  # for text mining
install.packages("SnowballC") # for text stemming
install.packages("wordcloud") # word-cloud generator 
install.packages("RColorBrewer") # color palettes
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")

docs <- Corpus(VectorSource(data))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))

# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)

set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

findFreqTerms(dtm, lowfreq = 4)



dtm <- TermDocumentMatrix(data)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
```









